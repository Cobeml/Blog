<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Cobe Liu">
<meta name="dcterms.date" content="2023-10-18">

<title>Blog - Picture Memory Review</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Picture Memory Review</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">memory</div>
                <div class="quarto-category">research</div>
                <div class="quarto-category">academic literature</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Cobe Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 18, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Picture Memory Review: Levie, W. H., &amp; Hathaway, S. N. (1988). Picture recognition memory: A review of research and theory. Journal of Visual Verbal Languaging, 8(1), 6–45.</p>
<section id="notes" class="level1">
<h1>Notes</h1>
<ul>
<li>Humans are generally good at picture recognition</li>
</ul>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">Experiments:</h2>
<ul>
<li>Subjects are shown set of images and then shown another set, partially containing previously shown images and the subject is to decide whether they have seen the picture before
<ul>
<li>Forced choice: test where subjects are shown several images and have to pick the one they have seen</li>
<li>Single item: subjects are shown images one at a time and subjects have to decide whether they have seen it before or not</li>
</ul></li>
</ul>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results:</h2>
<ul>
<li>Shepard (1967) displayed 612 color images and tested using two alternative force choice and median accuracy was 98.5%</li>
<li>Later experiments showed different but still astounding results when testing with different image set sizes.</li>
</ul>
</section>
<section id="pictures-vs.-words" class="level2">
<h2 class="anchored" data-anchor-id="pictures-vs.-words">Pictures vs.&nbsp;Words</h2>
<ul>
<li>Pictures are better remembered than words
<ul>
<li>pictoral superiority effect</li>
</ul></li>
<li>Pictogram memory even better than picture memory</li>
</ul>
</section>
<section id="pictures-and-words" class="level2">
<h2 class="anchored" data-anchor-id="pictures-and-words">Pictures and Words</h2>
<ul>
<li>Memory for pictures inc with presence of accompanying words</li>
<li>Experimenter provided words:
<ul>
<li>Memory inc when label is accurate and dec when label is inaccurate</li>
<li>Word specificy inc picture recognition</li>
<li>Words also inc aspects of picture that caption brings attention to</li>
</ul></li>
<li>Subject generated words:
<ul>
<li>verbal generated words will help picture recognition</li>
<li>Pictures with more similar descriptions are harder to decipher</li>
</ul></li>
</ul>
<section id="dual-vs-single-coded-memory-models" class="level3">
<h3 class="anchored" data-anchor-id="dual-vs-single-coded-memory-models">Dual vs Single Coded Memory Models</h3>
<ul>
<li>Paivio hypothysized that memory is stored in two places as nonverbal memory and verbal memory
<ul>
<li>Dual coded memory</li>
<li>Evidence to support includes isolations of use of either coding process with different results</li>
</ul></li>
<li>Single code memory: memory stored as abstract object in common spot, influenced by both verbal and nonverbal pathways</li>
<li>Some believe in possible triple coded memory models</li>
</ul>
</section>
</section>
<section id="distinctiveness" class="level2">
<h2 class="anchored" data-anchor-id="distinctiveness">Distinctiveness</h2>
<ul>
<li>How distinct a picture is increases memory for it</li>
<li>Unique images require more processing which would inc memory</li>
<li>May explain why picture memory is better than word memory: sensory input from words and therefore sensory memory (in dual code memory model) is less distinct that different pictures so makes words less memorable</li>
</ul>
<section id="general-vs-specific-memory" class="level3">
<h3 class="anchored" data-anchor-id="general-vs-specific-memory">General vs Specific Memory</h3>
<ul>
<li>Durso and associates believe that specificity help memory
<ul>
<li>words are less specific so less memorable</li>
</ul></li>
</ul>
</section>
</section>
<section id="meaningfulness" class="level2">
<h2 class="anchored" data-anchor-id="meaningfulness">Meaningfulness</h2>
<ul>
<li>Memory inc if image is more meaningful, and less random</li>
<li>If meaning is similar between pictures then memory dec</li>
</ul>
</section>
<section id="visual-richness" class="level2">
<h2 class="anchored" data-anchor-id="visual-richness">Visual Richness</h2>
<ul>
<li>Hypothesis: Pictures have more potential cues that could trigger the memory due to more richness</li>
<li>Nelson, Metzler, and Reed did experiment showing photos, detailed line drawings, and simple line drawings for 10 seconds each and memory for all types were about the same</li>
<li>Other experiments show that showing every 100-500 milliseconds made richness matter.</li>
<li>King (1986) study was that if recognition was performed immediately after, memory for abstract images and detailed phots was best but if tested a week later, memory for line drawing was best.</li>
</ul>
</section>
<section id="complexity" class="level2">
<h2 class="anchored" data-anchor-id="complexity">Complexity</h2>
<ul>
<li>To simulate more complex pictures, Pezdek (1978) as pictures with more figures
<ul>
<li>More figures meant short term worse but longer term better</li>
</ul></li>
<li>Generally mixed results in these studies</li>
</ul>
</section>
<section id="color" class="level2">
<h2 class="anchored" data-anchor-id="color">Color</h2>
<ul>
<li>Mixed results</li>
<li>College students color matters and not elementary</li>
<li>Color helps when people visualize black and white image filled with specific color of their choice and then shown image later with that color</li>
</ul>
</section>
<section id="others" class="level2">
<h2 class="anchored" data-anchor-id="others">Others</h2>
<ul>
<li>Figure ground separation is useful</li>
<li>Motion is useful</li>
<li>Brightness is useful</li>
</ul>
</section>
<section id="memory-for-parts" class="level2">
<h2 class="anchored" data-anchor-id="memory-for-parts">Memory for Parts</h2>
<ul>
<li>Memory is contained holistically and in detail</li>
<li>Detail is focused on points of interest</li>
<li>Images shown for short durations are more hollistically stored</li>
<li>Actors or Doers are more likely to be recognized than subjects in image who do nothing or are acted upon in images</li>
<li>Inventory information (objects in image)stays in memory for 4 months while descriptive goes away quick</li>
</ul>
<section id="orientation" class="level3">
<h3 class="anchored" data-anchor-id="orientation">Orientation</h3>
<ul>
<li>Mirror images are hard to spot unless flip is meaningful in some way</li>
</ul>
</section>
</section>
<section id="level-of-processing" class="level2">
<h2 class="anchored" data-anchor-id="level-of-processing">Level of Processing</h2>
<ul>
<li>Researchers manipulate depth of processing by asking questions about images
<ul>
<li>deep semantic processing caused by question like “is the scene located in the US?” is more beneficial to memory than shallow like “what colors are in the image?”</li>
</ul></li>
<li>Mental elaboration inc the memory to an extent
<ul>
<li>Work your mind has to do to fill in the blanks will increase memory as you had to elaborate more but has diminishing returns</li>
</ul></li>
<li>When asked to make character judgements of subjects of images, test subjects performed better at later memory than if they focused on details</li>
<li>Conceptual encoding by asking subject questions about object in photo’s function increases memory more than schematic encoding and acoustic encoding</li>
</ul>
</section>
<section id="presentation-duration" class="level2">
<h2 class="anchored" data-anchor-id="presentation-duration">Presentation Duration</h2>
<ul>
<li>Duration matters up to about 2 seconds depending on image complexity - after that there is a ceiling effect</li>
</ul>
</section>
<section id="interstimulus-interval" class="level2">
<h2 class="anchored" data-anchor-id="interstimulus-interval">Interstimulus interval</h2>
<ul>
<li>Time in between inc means inc memory</li>
<li>And visualizing image in that time also inc memory</li>
<li>When time between is varied with images, there is no difference in memory. This is because the picture rehearsal stage will not occur for as long as the break period as subjects are unsure how much time they have</li>
</ul>
</section>
<section id="test-delay" class="level2">
<h2 class="anchored" data-anchor-id="test-delay">Test Delay</h2>
<ul>
<li>Delay decreases the memory but a lot is still maintained long term</li>
</ul>
</section>
<section id="test-type-force-choice-v-single-item" class="level2">
<h2 class="anchored" data-anchor-id="test-type-force-choice-v-single-item">Test type (force choice v single item)</h2>
<ul>
<li>single item is harder as with force choice you can determine if you have seen one or have not seen one to determine the right choice</li>
<li>We are better at knowing when we have not seen something</li>
<li>In force choice, similar distractor image will dec performance</li>
<li>Single item performance is worse because of false negatives not false positives</li>
<li>More distractors means worse performance</li>
</ul>
</section>
<section id="test-modality-change" class="level2">
<h2 class="anchored" data-anchor-id="test-modality-change">Test modality change</h2>
<ul>
<li>Better if train and test are the same modality</li>
</ul>
</section>
<section id="individual-differences" class="level2">
<h2 class="anchored" data-anchor-id="individual-differences">Individual differences</h2>
<ul>
<li>Not big differences across people in the same age group</li>
<li>More difference comes with recognizing faces than other types of images</li>
</ul>
<section id="age-group-differences" class="level3">
<h3 class="anchored" data-anchor-id="age-group-differences">Age group differences</h3>
<ul>
<li>Performance generally inc with age</li>
<li>Older people are more likely to make inferences based on seeing pictures they have never seen before</li>
<li>Even older people with memory issues do not decline and even improve in picture memory</li>
</ul>
</section>
<section id="sex-differences" class="level3">
<h3 class="anchored" data-anchor-id="sex-differences">Sex Differences</h3>
<ul>
<li>Women generally better at faces</li>
</ul>
</section>
</section>
<section id="experience-with-class-of-stimuli" class="level2">
<h2 class="anchored" data-anchor-id="experience-with-class-of-stimuli">Experience with class of stimuli</h2>
<ul>
<li>People better with faces of their own race
<ul>
<li>Not true with children</li>
</ul></li>
</ul>
</section>
<section id="face-recognition" class="level2">
<h2 class="anchored" data-anchor-id="face-recognition">Face Recognition</h2>
<ul>
<li>People are very good at remembering faces</li>
<li>More detailed the photo the better</li>
<li>Even if the face changes (the person aged) recognition only drops a little</li>
<li>Left face better for recognition that the right half</li>
<li>Possibly different memory pathway</li>
</ul>
</section>
<section id="eye-witness-research" class="level2">
<h2 class="anchored" data-anchor-id="eye-witness-research">Eye Witness Research</h2>
<ul>
<li>Witness of crimes can recognize perpetrator above chance.</li>
<li>Confidence does not affect performance in selecting perpetrator</li>
<li>When people are allowed to say I don’t know, false positives go down and true positives remain constant.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>