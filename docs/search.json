[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/picture_memory/index.html",
    "href": "posts/picture_memory/index.html",
    "title": "Picture Memory",
    "section": "",
    "text": "Here are two picture memory experiments. One manipulates the duration that the picture was initally displayed and the other shows each image with either a red, green, yellow, blue, or purple and manipulates if the image was shown again with the same color background.\nPicture Memory with Manipulated Duration\nPicture Memory with Manipulated Colored Background Consistency"
  },
  {
    "objectID": "posts/data-analysis/index.html",
    "href": "posts/data-analysis/index.html",
    "title": "Data Analysis",
    "section": "",
    "text": "Downloading Data\n\n# install from the Packages tab, or run the below in the console once.\n#install.packages('tidyverse')\n#install.packages('rio')\n\n# load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rio)\n\n# get data file names\nfile_names &lt;- list.files(\"sample_data\",full.names = TRUE)\n\n# initialize data frame to hold individual subject data\nall_data &lt;- tibble()\n\n# loop through each file and import\nfor(i in 1:length(file_names)) {\n  \n  # import a single data file to a temporary data frame\n  temp_df &lt;- rio::import(file_names[i]) %&gt;%\n    mutate(subject = i)\n  \n  # append the single subject data to larger data frame\n  all_data &lt;- rbind(all_data,temp_df)\n  \n}\n\nAnalysis of sample data analyzing reaction time differences between incongruent and congruent trials:\n\n# pre-process and filter rows\nfiltered_data &lt;- all_data %&gt;%\n  filter(task == \"stroop\",\n         correct == \"TRUE\") %&gt;%\n  mutate(rt = as.numeric(rt))\n\n# get individual subject means in each condition\nsubject_mean_RT &lt;- filtered_data %&gt;%\n  group_by(subject,congruency) %&gt;%\n  summarize(mean_rt = mean(rt), .groups = \"drop\")\n\n# get group means in each condition\ngroup_mean_RT &lt;- subject_mean_RT %&gt;%\n  group_by(congruency) %&gt;%\n  summarize(mean_reaction_time = mean(mean_rt),\n            sem = sd(mean_rt)/sqrt(length(mean_rt))\n            )\n\n# plot\nggplot(group_mean_RT, aes(x=congruency,y=mean_reaction_time)) +\n  geom_bar(stat=\"identity\") +\n  geom_errorbar(aes(ymin=mean_reaction_time-sem,\n                    ymax=mean_reaction_time+sem),\n                width=.2) +\n  ylab(\"Mean Reaction Time (ms)\") +\n  xlab(\"Congruency\")+\n  coord_cartesian(ylim=c(400,800)) +\n  theme_classic()\n\n\n\n\nAnalysis of sample data analyzing error rate differences between congruent and incongruent trials:\n\n# pre-process and filter rows\nfiltered_data_pc &lt;- all_data %&gt;%\n  filter(task == \"stroop\")\n\n# get individual subject proportion correct values\nsubject_pc &lt;- filtered_data_pc %&gt;%\n  group_by(subject,congruency) %&gt;%\n  summarize(proportion_correct = mean(correct), .groups = \"drop\")\n\n# get group means in each condition\ngroup_mean_pc &lt;- subject_pc %&gt;%\n  group_by(congruency) %&gt;%\n  summarize(mean_proportion_correct = mean(proportion_correct),\n            sem = sd(proportion_correct)/sqrt(length(proportion_correct))\n            )\n\n# plot\nggplot(group_mean_pc, aes(x=congruency,y=mean_proportion_correct)) +\n  geom_bar(stat=\"identity\") +\n  geom_errorbar(aes(ymin=mean_proportion_correct-sem,\n                    ymax=mean_proportion_correct+sem),\n                width=.2) +\n  ylab(\"Mean Proportion Correct\") +\n  xlab(\"Congruency\")+\n  coord_cartesian(ylim=c(0.5,1)) +\n  theme_classic()"
  },
  {
    "objectID": "posts/data-analysis/index.html#sample-data-analusis",
    "href": "posts/data-analysis/index.html#sample-data-analusis",
    "title": "Data Analysis",
    "section": "",
    "text": "Downloading Data\n\n# install from the Packages tab, or run the below in the console once.\n#install.packages('tidyverse')\n#install.packages('rio')\n\n# load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rio)\n\n# get data file names\nfile_names &lt;- list.files(\"sample_data\",full.names = TRUE)\n\n# initialize data frame to hold individual subject data\nall_data &lt;- tibble()\n\n# loop through each file and import\nfor(i in 1:length(file_names)) {\n  \n  # import a single data file to a temporary data frame\n  temp_df &lt;- rio::import(file_names[i]) %&gt;%\n    mutate(subject = i)\n  \n  # append the single subject data to larger data frame\n  all_data &lt;- rbind(all_data,temp_df)\n  \n}\n\nAnalysis of sample data analyzing reaction time differences between incongruent and congruent trials:\n\n# pre-process and filter rows\nfiltered_data &lt;- all_data %&gt;%\n  filter(task == \"stroop\",\n         correct == \"TRUE\") %&gt;%\n  mutate(rt = as.numeric(rt))\n\n# get individual subject means in each condition\nsubject_mean_RT &lt;- filtered_data %&gt;%\n  group_by(subject,congruency) %&gt;%\n  summarize(mean_rt = mean(rt), .groups = \"drop\")\n\n# get group means in each condition\ngroup_mean_RT &lt;- subject_mean_RT %&gt;%\n  group_by(congruency) %&gt;%\n  summarize(mean_reaction_time = mean(mean_rt),\n            sem = sd(mean_rt)/sqrt(length(mean_rt))\n            )\n\n# plot\nggplot(group_mean_RT, aes(x=congruency,y=mean_reaction_time)) +\n  geom_bar(stat=\"identity\") +\n  geom_errorbar(aes(ymin=mean_reaction_time-sem,\n                    ymax=mean_reaction_time+sem),\n                width=.2) +\n  ylab(\"Mean Reaction Time (ms)\") +\n  xlab(\"Congruency\")+\n  coord_cartesian(ylim=c(400,800)) +\n  theme_classic()\n\n\n\n\nAnalysis of sample data analyzing error rate differences between congruent and incongruent trials:\n\n# pre-process and filter rows\nfiltered_data_pc &lt;- all_data %&gt;%\n  filter(task == \"stroop\")\n\n# get individual subject proportion correct values\nsubject_pc &lt;- filtered_data_pc %&gt;%\n  group_by(subject,congruency) %&gt;%\n  summarize(proportion_correct = mean(correct), .groups = \"drop\")\n\n# get group means in each condition\ngroup_mean_pc &lt;- subject_pc %&gt;%\n  group_by(congruency) %&gt;%\n  summarize(mean_proportion_correct = mean(proportion_correct),\n            sem = sd(proportion_correct)/sqrt(length(proportion_correct))\n            )\n\n# plot\nggplot(group_mean_pc, aes(x=congruency,y=mean_proportion_correct)) +\n  geom_bar(stat=\"identity\") +\n  geom_errorbar(aes(ymin=mean_proportion_correct-sem,\n                    ymax=mean_proportion_correct+sem),\n                width=.2) +\n  ylab(\"Mean Proportion Correct\") +\n  xlab(\"Congruency\")+\n  coord_cartesian(ylim=c(0.5,1)) +\n  theme_classic()"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html",
    "href": "posts/picture-memory-literature/index.html",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Levie, W. H., & Hathaway, S. N. (1988). Picture recognition memory: A review of research and theory. Journal of Visual Verbal Languaging, 8(1), 6–45. - Humans are generally good at picture recognition\n\n\n\nSubjects are shown set of images and then shown another set, partially containing previously shown images and the subject is to decide whether they have seen the picture before\n\nForced choice: test where subjects are shown several images and have to pick the one they have seen\nSingle item: subjects are shown images one at a time and subjects have to decide whether they have seen it before or not\n\n\n\n\n\n\nShepard (1967) displayed 612 color images and tested using two alternative force choice and median accuracy was 98.5%\nLater experiments showed different but still astounding results when testing with different image set sizes.\n\n\n\n\n\nPictures are better remembered than words\n\npictoral superiority effect\n\nPictogram memory even better than picture memory\n\n\n\n\n\nMemory for pictures inc with presence of accompanying words\nExperimenter provided words:\n\nMemory inc when label is accurate and dec when label is inaccurate\nWord specificy inc picture recognition\nWords also inc aspects of picture that caption brings attention to\n\nSubject generated words:\n\nverbal generated words will help picture recognition\nPictures with more similar descriptions are harder to decipher\n\n\n\n\n\nPaivio hypothysized that memory is stored in two places as nonverbal memory and verbal memory\n\nDual coded memory\nEvidence to support includes isolations of use of either coding process with different results\n\nSingle code memory: memory stored as abstract object in common spot, influenced by both verbal and nonverbal pathways\nSome believe in possible triple coded memory models\n\n\n\n\n\n\nHow distinct a picture is increases memory for it\nUnique images require more processing which would inc memory\nMay explain why picture memory is better than word memory: sensory input from words and therefore sensory memory (in dual code memory model) is less distinct that different pictures so makes words less memorable\n\n\n\n\nDurso and associates believe that specificity help memory\n\nwords are less specific so less memorable\n\n\n\n\n\n\n\nMemory inc if image is more meaningful, and less random\nIf meaning is similar between pictures then memory dec\n\n\n\n\n\nHypothesis: Pictures have more potential cues that could trigger the memory due to more richness\nNelson, Metzler, and Reed did experiment showing photos, detailed line drawings, and simple line drawings for 10 seconds each and memory for all types were about the same\nOther experiments show that showing every 100-500 milliseconds made richness matter.\nKing (1986) study was that if recognition was performed immediately after, memory for abstract images and detailed phots was best but if tested a week later, memory for line drawing was best.\n\n\n\n\n\nTo simulate more complex pictures, Pezdek (1978) as pictures with more figures\n\nMore figures meant short term worse but longer term better\n\nGenerally mixed results in these studies\n\n\n\n\n\nMixed results\nCollege students color matters and not elementary\nColor helps when people visualize black and white image filled with specific color of their choice and then shown image later with that color\n\n\n\n\n\nFigure ground separation is useful\nMotion is useful\nBrightness is useful\n\n\n\n\n\nMemory is contained holistically and in detail\nDetail is focused on points of interest\nImages shown for short durations are more hollistically stored\nActors or Doers are more likely to be recognized than subjects in image who do nothing or are acted upon in images\nInventory information (objects in image)stays in memory for 4 months while descriptive goes away quick\n\n\n\n\nMirror images are hard to spot unless flip is meaningful in some way\n\n\n\n\n\n\nResearchers manipulate depth of processing by asking questions about images\n\ndeep semantic processing caused by question like “is the scene located in the US?” is more beneficial to memory than shallow like “what colors are in the image?”\n\nMental elaboration inc the memory to an extent\n\nWork your mind has to do to fill in the blanks will increase memory as you had to elaborate more but has diminishing returns\n\nWhen asked to make character judgements of subjects of images, test subjects performed better at later memory than if they focused on details\nConceptual encoding by asking subject questions about object in photo’s function increases memory more than schematic encoding and acoustic encoding\n\n\n\n\n\nDuration matters up to about 2 seconds depending on image complexity - after that there is a ceiling effect\n\n\n\n\n\nTime in between inc means inc memory\nAnd visualizing image in that time also inc memory\nWhen time between is varied with images, there is no difference in memory. This is because the picture rehearsal stage will not occur for as long as the break period as subjects are unsure how much time they have\n\n\n\n\n\nDelay decreases the memory but a lot is still maintained long term\n\n\n\n\n\nsingle item is harder as with force choice you can determine if you have seen one or have not seen one to determine the right choice\nWe are better at knowing when we have not seen something\nIn force choice, similar distractor image will dec performance\nSingle item performance is worse because of false negatives not false positives\nMore distractors means worse performance\n\n\n\n\n\nBetter if train and test are the same modality\n\n\n\n\n\nNot big differences across people in the same age group\nMore difference comes with recognizing faces than other types of images\n\n\n\n\nPerformance generally inc with age\nOlder people are more likely to make inferences based on seeing pictures they have never seen before\nEven older people with memory issues do not decline and even improve in picture memory\n\n\n\n\n\nWomen generally better at faces\n\n\n\n\n\n\nPeople better with faces of their own race\n\nNot true with children\n\n\n\n\n\n\nPeople are very good at remembering faces\nMore detailed the photo the better\nEven if the face changes (the person aged) recognition only drops a little\nLeft face better for recognition that the right half\nPossibly different memory pathway\n\n\n\n\n\nWitness of crimes can recognize perpetrator above chance.\nConfidence does not affect performance in selecting perpetrator\nWhen people are allowed to say I don’t know, false positives go down and true positives remain constant."
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#experiments",
    "href": "posts/picture-memory-literature/index.html#experiments",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Subjects are shown set of images and then shown another set, partially containing previously shown images and the subject is to decide whether they have seen the picture before\n\nForced choice: test where subjects are shown several images and have to pick the one they have seen\nSingle item: subjects are shown images one at a time and subjects have to decide whether they have seen it before or not"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#results",
    "href": "posts/picture-memory-literature/index.html#results",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Shepard (1967) displayed 612 color images and tested using two alternative force choice and median accuracy was 98.5%\nLater experiments showed different but still astounding results when testing with different image set sizes."
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#pictures-vs.-words",
    "href": "posts/picture-memory-literature/index.html#pictures-vs.-words",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Pictures are better remembered than words\n\npictoral superiority effect\n\nPictogram memory even better than picture memory"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#pictures-and-words",
    "href": "posts/picture-memory-literature/index.html#pictures-and-words",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Memory for pictures inc with presence of accompanying words\nExperimenter provided words:\n\nMemory inc when label is accurate and dec when label is inaccurate\nWord specificy inc picture recognition\nWords also inc aspects of picture that caption brings attention to\n\nSubject generated words:\n\nverbal generated words will help picture recognition\nPictures with more similar descriptions are harder to decipher\n\n\n\n\n\nPaivio hypothysized that memory is stored in two places as nonverbal memory and verbal memory\n\nDual coded memory\nEvidence to support includes isolations of use of either coding process with different results\n\nSingle code memory: memory stored as abstract object in common spot, influenced by both verbal and nonverbal pathways\nSome believe in possible triple coded memory models"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#distinctiveness",
    "href": "posts/picture-memory-literature/index.html#distinctiveness",
    "title": "Picture Memory Review",
    "section": "",
    "text": "How distinct a picture is increases memory for it\nUnique images require more processing which would inc memory\nMay explain why picture memory is better than word memory: sensory input from words and therefore sensory memory (in dual code memory model) is less distinct that different pictures so makes words less memorable\n\n\n\n\nDurso and associates believe that specificity help memory\n\nwords are less specific so less memorable"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#meaningfulness",
    "href": "posts/picture-memory-literature/index.html#meaningfulness",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Memory inc if image is more meaningful, and less random\nIf meaning is similar between pictures then memory dec"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#visual-richness",
    "href": "posts/picture-memory-literature/index.html#visual-richness",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Hypothesis: Pictures have more potential cues that could trigger the memory due to more richness\nNelson, Metzler, and Reed did experiment showing photos, detailed line drawings, and simple line drawings for 10 seconds each and memory for all types were about the same\nOther experiments show that showing every 100-500 milliseconds made richness matter.\nKing (1986) study was that if recognition was performed immediately after, memory for abstract images and detailed phots was best but if tested a week later, memory for line drawing was best."
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#complexity",
    "href": "posts/picture-memory-literature/index.html#complexity",
    "title": "Picture Memory Review",
    "section": "",
    "text": "To simulate more complex pictures, Pezdek (1978) as pictures with more figures\n\nMore figures meant short term worse but longer term better\n\nGenerally mixed results in these studies"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#color",
    "href": "posts/picture-memory-literature/index.html#color",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Mixed results\nCollege students color matters and not elementary\nColor helps when people visualize black and white image filled with specific color of their choice and then shown image later with that color"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#others",
    "href": "posts/picture-memory-literature/index.html#others",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Figure ground separation is useful\nMotion is useful\nBrightness is useful"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#memory-for-parts",
    "href": "posts/picture-memory-literature/index.html#memory-for-parts",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Memory is contained holistically and in detail\nDetail is focused on points of interest\nImages shown for short durations are more hollistically stored\nActors or Doers are more likely to be recognized than subjects in image who do nothing or are acted upon in images\nInventory information (objects in image)stays in memory for 4 months while descriptive goes away quick\n\n\n\n\nMirror images are hard to spot unless flip is meaningful in some way"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#level-of-processing",
    "href": "posts/picture-memory-literature/index.html#level-of-processing",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Researchers manipulate depth of processing by asking questions about images\n\ndeep semantic processing caused by question like “is the scene located in the US?” is more beneficial to memory than shallow like “what colors are in the image?”\n\nMental elaboration inc the memory to an extent\n\nWork your mind has to do to fill in the blanks will increase memory as you had to elaborate more but has diminishing returns\n\nWhen asked to make character judgements of subjects of images, test subjects performed better at later memory than if they focused on details\nConceptual encoding by asking subject questions about object in photo’s function increases memory more than schematic encoding and acoustic encoding"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#presentation-duration",
    "href": "posts/picture-memory-literature/index.html#presentation-duration",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Duration matters up to about 2 seconds depending on image complexity - after that there is a ceiling effect"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#interstimulus-interval",
    "href": "posts/picture-memory-literature/index.html#interstimulus-interval",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Time in between inc means inc memory\nAnd visualizing image in that time also inc memory\nWhen time between is varied with images, there is no difference in memory. This is because the picture rehearsal stage will not occur for as long as the break period as subjects are unsure how much time they have"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#test-delay",
    "href": "posts/picture-memory-literature/index.html#test-delay",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Delay decreases the memory but a lot is still maintained long term"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#test-type-force-choice-v-single-item",
    "href": "posts/picture-memory-literature/index.html#test-type-force-choice-v-single-item",
    "title": "Picture Memory Review",
    "section": "",
    "text": "single item is harder as with force choice you can determine if you have seen one or have not seen one to determine the right choice\nWe are better at knowing when we have not seen something\nIn force choice, similar distractor image will dec performance\nSingle item performance is worse because of false negatives not false positives\nMore distractors means worse performance"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#test-modality-change",
    "href": "posts/picture-memory-literature/index.html#test-modality-change",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Better if train and test are the same modality"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#individual-differences",
    "href": "posts/picture-memory-literature/index.html#individual-differences",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Not big differences across people in the same age group\nMore difference comes with recognizing faces than other types of images\n\n\n\n\nPerformance generally inc with age\nOlder people are more likely to make inferences based on seeing pictures they have never seen before\nEven older people with memory issues do not decline and even improve in picture memory\n\n\n\n\n\nWomen generally better at faces"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#experience-with-class-of-stimuli",
    "href": "posts/picture-memory-literature/index.html#experience-with-class-of-stimuli",
    "title": "Picture Memory Review",
    "section": "",
    "text": "People better with faces of their own race\n\nNot true with children"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#face-recognition",
    "href": "posts/picture-memory-literature/index.html#face-recognition",
    "title": "Picture Memory Review",
    "section": "",
    "text": "People are very good at remembering faces\nMore detailed the photo the better\nEven if the face changes (the person aged) recognition only drops a little\nLeft face better for recognition that the right half\nPossibly different memory pathway"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#eye-witness-research",
    "href": "posts/picture-memory-literature/index.html#eye-witness-research",
    "title": "Picture Memory Review",
    "section": "",
    "text": "Witness of crimes can recognize perpetrator above chance.\nConfidence does not affect performance in selecting perpetrator\nWhen people are allowed to say I don’t know, false positives go down and true positives remain constant."
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#emotional-response---previous-research-on-inc-hit-rates",
    "href": "posts/picture-memory-literature/index.html#emotional-response---previous-research-on-inc-hit-rates",
    "title": "Picture Memory Review",
    "section": "Emotional Response - Previous Research on Inc Hit Rates",
    "text": "Emotional Response - Previous Research on Inc Hit Rates\n\nArousing emotional response can be measured by stress hormone release which activates the amygdala\nAmygdala activation modulates memory areas of brain (medial temporal lobe and hippocampus)\n\nthis increases hit rates and recognition\n\nNon arousal emotional response comes in the prefrontal hippocampus\n\nAlso leads to better hit rates\n\nNegative arousing is better for memory than any other emotional response"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#research-on-false-positives",
    "href": "posts/picture-memory-literature/index.html#research-on-false-positives",
    "title": "Picture Memory Review",
    "section": "Research on False Positives",
    "text": "Research on False Positives\n\nAlso might increase false positives\n\nLots of research done with words that confirms this\nIs it true with images?\n\n\n\nFuzzy trace theory\n\nTwo types of memory stored with emotional picture\n\nverbatim trace with details\ngist trace with meaning\n\nInc in Emotion (mostly negative) inc gist memory\nPositive response might inc verbatim memory\nverbatim memory and performance are positively correlated\ngist memory and false positives are positively correlated\n\n\n\nAmygdala Response pics v words\n\nAmygdala response with words is more right side which is more gist\nwith pics in bilateral which may inc verbatim\ngist memory may not affect hit rates"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#experiment-1",
    "href": "posts/picture-memory-literature/index.html#experiment-1",
    "title": "Picture Memory Review",
    "section": "Experiment 1",
    "text": "Experiment 1\n\n65 college students, 5 were excluded due to being outliers in picture recognition and bias estimates\nPicked 270 pictures, divided into 90 positive, 90 neutral, 90 negative, no extreme photos\n\n\nExperiment 1a low arousal controlled content\n\ncontrolled for relatedness by including images from groups with images based on amount of humans, and amount of animals in picture\n\nequated amount from each group in each positive, negative, and neutral section\n\n\n\n\nExperiment 1b low arousal uncontrolled content\n\ncontent not controlled\n\n\n\nResults\n\nHit rate increases with less control\nNo effect of content on emotion\nemotion did not effect hit rates\nemotion did effect false positives\nemotion did effect acuracy - lower accuracy for negative pictures"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#experiment-2",
    "href": "posts/picture-memory-literature/index.html#experiment-2",
    "title": "Picture Memory Review",
    "section": "Experiment 2",
    "text": "Experiment 2\n\nSame Process except include extreme pictures including mutilation\nSame control and uncontrolled groups\n\n\nResults\n\nEmotion increased the false positives\nWhen content is controlled, positive had greatest false alarms and neutral had least\nWhen content is uncontrolled, positive and negative false alarm rates are the same.\nemotion reduces everything"
  },
  {
    "objectID": "posts/picture-memory-literature/index.html#experiment-3---delayed-testing-one-week-after-encoding",
    "href": "posts/picture-memory-literature/index.html#experiment-3---delayed-testing-one-week-after-encoding",
    "title": "Picture Memory Review",
    "section": "Experiment 3 - Delayed testing, one week after encoding",
    "text": "Experiment 3 - Delayed testing, one week after encoding\n\nGreater hit rates for negative\nTested relatedness effect and also"
  },
  {
    "objectID": "posts/picture-orientation/index.html",
    "href": "posts/picture-orientation/index.html",
    "title": "Image Orientation",
    "section": "",
    "text": "Scapinello, K. F., & Yarmey, A. D. (1970). The role of familiarity and orientation in immediate and delayed recognition of pictorial stimuli. Psychonomic Science, 21(6), 329–330. - The authors used three types of stimuli: human faces, canine faces, and buildings. They manipulated the familiarity (presented once or seven times) and the orientation (upright or rotated) of the stimuli in a part-to-whole transfer paradigm. - Findings: * Familiar stimuli were better recognized than unfamiliar stimuli, regardless of orientation and recall interval. * Recognition performance declined with stimulus rotation especially for human faces. * The decline for human faces was independent of familiarity, suggesting a face-specific factor that makes them more sensitive to rotation. * The authors speculated that the face-specific factor might be related to how the face cues are more relliant on orientation.\nVogel, Juliet M. “The Influence of Verbal Descriptions versus Orientation Codes on Kindergartners’ Memory for the Orientation of Pictures.” Child Development, vol. 50, no. 1, Mar. 1979, pp. 239–42. EBSCOhost, https://doi.org/10.2307/1129062."
  },
  {
    "objectID": "posts/picture-memorability/index.html",
    "href": "posts/picture-memorability/index.html",
    "title": "Picture Memorability",
    "section": "",
    "text": "Isola, P., Xiao, J., Torralba, A., & Oliva, A. (2011). What makes an image memorable? Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference On, 145–152.\n\ninvestigates the problem of predicting how memorable an image will be, what image features and labels contribute to memorability\nused database of 2222 images with memorability scores measured by a visual memory game on Amazon Mechanical Turk. Participants had to detect repeated images in a stream of pictures\nThe paper analyzes the consistency of image memorability across different viewers, and finds a correlation between memorability scores measured on independent sets of participants\np they cite is not a p-value but a measure of correlation called Spearman score\nThe paper explores general non-semantic global features and semantic features and explores how they relate to memorability\nnon-semantic features were not predictive (low correlation)\nthe number of labled objects or labled spaces (the amount of semantics in the image) correlates better with image memorability\ndifferent objects increase memorability differently (people, cars, seating, and floors score well and buildings, ceilings, skies, trees, and mountains score low)\nThey also compare models they created that take many global features into account to predict image memorability. The global feature aggregates also include features that indicate how many labled objects there are. This model performed well and performed better when combined with Labeled Multiscale Object Areas and Scene Categories\n\n\nExperiment Proposal\nWe could show objects in which the images are from different categories and subjects are shown images from these sets but with different distributions from each set.\nWe could show pictures with objects that do not often go together and ones that have objects that go together and see if their model is still predictive. It probably would not be becase their model does not take the relationship of objects into account. One way to simulate this would be to map the how different combinations of objects are linked to memory."
  },
  {
    "objectID": "jspsych/contributors.html",
    "href": "jspsych/contributors.html",
    "title": "Blog",
    "section": "",
    "text": "The following people have contributed to the development of jsPsych by writing code, documentation, and/or suggesting improvements (in alphabetical order): * alisdt - https://github.com/alisdt * Antonia - https://github.com/Ahoidal * aucuparia - https://github.com/aucuparia * Xiaolu Bai - https://github.com/lbai001 * bjoluc - https://github.com/bjoluc * Christian Brickhouse - https://github.com/chrisbrickhouse * Teon L Brooks - https://github.com/teonbrooks * Eamon Caddigan - https://github.com/eamoncaddigan * Jason Carpenter * Steve Chao - https://github.com/stchao * Zhanwen “Phil” Chen - https://github.com/zhanwenchen * cthorey - https://github.com/cthorey * Guy Davidson - https://github.com/guydav * Kristin Diep - https://github.com/kristiyip * Ari Dyckovsky - https://github.com/aridyckovsky * Etienne Gaudrain - https://github.com/egaudrain * Jon Gauthier - https://github.com/hans * Robert Gibboni - https://github.com/r-b-g-b * Becky Gilbert - https://github.com/becky-gilbert * Mark Gorenstein - https://github.com/mgorenstein * Rui Han - https://github.com/hrcn * Andy Heusser - https://github.com/andrewheusser * Angus Hughes - https://github.com/awhug * jadeddelta - https://github.com/jadeddelta * Gustavo Juantorena - https://github.com/GEJ1 * Chris Jungerius - https://github.com/cjungerius * George Kachergis - https://github.com/kachergis * Yul Kang - https://github.com/yulkang * Spencer King - https://github.com/spencerking * Jana Klaus - https://github.com/janakl4us * Arnold Kochari - https://github.com/akochari * Peter Jes Kohler - https://github.com/pjkohler * kupiqu - https://github.com/kupiqu * Daiichiro Kuroki - https://github.com/kurokida * Jonas Lambers * madebyafox - https://github.com/madebyafox * Shane Martin - https://github.com/shamrt * Vijay Marupudi - https://github.com/vijaymarupudi * Adrian Oesch - https://github.com/adrianoesch * Benjamin Ooghe-Tabanou - https://github.com/boogheta * Nikolay B Petrov - https://github.com/nikbpetrov * Dillon Plunkett - https://github.com/dillonplunkett * Junyan Qi - https://github.com/GavinQ1 * Sivananda Rajananda - https://github.com/vrsivananda * Dan Rivas - https://github.com/rivasd * Werner Sævland - https://github.com/wernersa * Marian Sauter - https://github.com/mariansauter * Ellen Shapiro - https://github.com/designatednerd * Jan Simson - https://github.com/jansim * Hannah Small - https://github.com/hesmall * sprengholz - https://github.com/sprengholz * Dominik Strohmeier - https://github.com/onkeltom * Nabeel Sulieman - https://github.com/nabsul * Hitoshi Tominaga - https://github.com/tbrotherm * Tim Vergenz - https://github.com/vergenzt * Matteo Visconti di Oleggio Castello - https://github.com/mvdoc * Ilya Vorontsov - https://github.com/VorontsovIE * Wolfgang Walther - https://github.com/wolfgangwalther * Erik Weitnauer - https://github.com/eweitnauer * Rob Wilkinson - https://github.com/RobAWilkinson * Andy Woods - https://github.com/andytwoods * Reto Wyss - https://github.com/retowyss"
  },
  {
    "objectID": "jspsych/code-of-conduct.html",
    "href": "jspsych/code-of-conduct.html",
    "title": "jsPsych Code of Conduct",
    "section": "",
    "text": "jsPsych aims to provide a productive, helpful, and agile community that: * Welcomes new users and ideas * Seeks to improve the jsPsych library for all users * Works toward best scientific practices * Ensures that jsPsych remains as accessible as possible\nWe hope that this community will encourage collaboration between users/groups with different needs, interests, and skills.\njsPsych was created with the goal of making web-based behavioral experiments as easy as possible for those who do not have a background in web development or any other kind of computer programming. Thus, novice users are an essential part of this community. The involvement of beginner programmers is not only welcome, but critical for the continued success of the project.\nSimilarly, a core aim of the jsPsych project is to make web-based behavioral experiments accessible to anyone who wishes to utilize them. We can better achieve this goal with diverse voices guiding the project, and we value participation from those who bring different perspectives to our community. We actively encourage participation from those who come from historically underrepresented groups in software development and STEM.\n\n\n\nBe friendly and patient: Remember that you might not be communicating in someone else’s primary spoken or programming language, and that we don’t all have the same background knowledge and experience.\nBe helpful: Offer constructive feedback on others’ ideas and code.\nBe collaborative: Collaboration improves the efficiency and quality of our work. We aim to understand the needs of other users, and to work transparently with others to coordinate our efforts.\nBe considerate and respectful: There is no excuse for disrespectful behavior. When making comments in public spaces, we should consider the potential impact on the whole community. For instance, jokes/teasing may seem harmless among users who are familiar with one another, but this can be intimidating and off-putting for other community members.\nAsk for help and seek feedback: We’re all learning, and no one is expected to be perfect. Asking questions and seeking feedback early on can help us work efficiently and avoid bigger problems in the future.\nTake responsibility for your actions: We all make mistakes; when we do, we take responsibility for them. If someone has been harmed or offended, we listen carefully and respectfully, and work to improve our behavior.\nValue decisiveness, clarity, and consensus: We hope that jsPsych contributors can resolve disagreements constructively, using clear rationale for their perspective, and with humility. When they cannot, the project leader can offer clarity and direction.\n\n\n\n\nThe community’s core principles apply to all private and public spaces where the jsPsych project is officially managed, promoted, or discussed. Examples include all of the jsPsych-related GitHub repos (issues, commit messages, pull requests, etc.), the jsPsych Google Group, and anyone acting as a representative of the project, whether in person or online, including via social media and email. We expect this Code of Conduct to be honoured by everyone in the jsPsych community.\n\n\n\n\nWe encourage all participants in the jsPsych community to feel empowered to lead, to take action, and to experiment when they feel that innovation could improve the project. There is no need to wait for delegation or permission - leadership can be exercised by anyone simply by taking action.\nAlthough we strive to make jsPsych as open and collaborative as possible, it’s important to have a project leader in order to ensure that jsPsych remains (1) as accessible as possible for all users, (2) stable and reliable, (3) easy to maintain, (4) internally consistent, etc.\nSuch leadership occasionally requires decisions that are in the best interest of the project but may not be understood by or beneficial to everyone. These decisions are important because they keep the project on track, and enable it to move forward faster than if we required complete consensus. Any major decisions taken by the project leader should be mindful of the challenges they may present for others. We expect the project leader to communicate major decisions/changes early on, and to provide their reasoning.\n\n\nWe invite anyone to contribute to any aspect of the project. You can find more information about contributing here: https://www.jspsych.org/latest/developers/contributing/\nBecause changes to the jsPsych library can have a huge potential for impact on all users, any proposed changes to the library have to be considered very carefully. For this reason, it is always a good idea to check with the project leader about whether your proposed contribution would likely be merged into the jsPsych library. Keep in mind that, even if your work isn’t suitable for merging into the jsPsych library, you can still use it in your own projects and share it publicly with others, e.g. via a separate GitHub repository.\nContributors are acknowledged in the contributors.md file in the jsPsych GitHub repo, and in the notes for each release. If you feel that your contribution has been overlooked, please contact Josh de Leeuw (josh.deleeuw@gmail.com). We also encourage opening a pull request to modify the contributors.md file.\n\n\n\nWe expect jsPsych users and developers to be aware when they are conflicted due to employment or other projects they are involved in, and to abstain or delegate decisions that may be seen to be self-interested. We expect that everyone who participates in the jsPsych community does so with the goal of improving the library for all of its users.\n\n\n\n\nIn the event that a Code of Conduct issue cannot be resolved among users, please feel free to report your concerns to Josh de Leeuw (josh.deleeuw@gmail.com). If Josh de Leeuw is involved in the issue, you can report it to Becky Gilbert instead (beckyannegilbert@gmail.com). In your report, please include as much information as possible about what occurred, including any relevant links/attachments.\nAll reports will be reviewed by a multi-person team and will result in a response that is deemed necessary and appropriate to the circumstances. Where additional perspectives are needed, the team may seek insight from others with relevant expertise or experience. The confidentiality of the person reporting the incident will be kept at all times. Involved parties are never part of the review team. Anyone asked to stop unacceptable behavior is expected to comply immediately. If an individual engages in unacceptable behavior, the review team may take any action they deem appropriate, including a permanent ban from the community.\nThis code is not exhaustive or complete. It serves to capture our common understanding of a welcoming, productive, and collaborative environment. We expect the code to be followed in spirit as much as in the letter.\n\n\n\nThis document was based on, or inspired by, the following Codes of Conduct: * Ubuntu https://ubuntu.com/community/code-of-conduct * Microsoft https://opensource.microsoft.com/codeofconduct/ * Contributor Covenant https://www.contributor-covenant.org/version/1/4/code-of-conduct"
  },
  {
    "objectID": "jspsych/code-of-conduct.html#community",
    "href": "jspsych/code-of-conduct.html#community",
    "title": "jsPsych Code of Conduct",
    "section": "",
    "text": "jsPsych aims to provide a productive, helpful, and agile community that: * Welcomes new users and ideas * Seeks to improve the jsPsych library for all users * Works toward best scientific practices * Ensures that jsPsych remains as accessible as possible\nWe hope that this community will encourage collaboration between users/groups with different needs, interests, and skills.\njsPsych was created with the goal of making web-based behavioral experiments as easy as possible for those who do not have a background in web development or any other kind of computer programming. Thus, novice users are an essential part of this community. The involvement of beginner programmers is not only welcome, but critical for the continued success of the project.\nSimilarly, a core aim of the jsPsych project is to make web-based behavioral experiments accessible to anyone who wishes to utilize them. We can better achieve this goal with diverse voices guiding the project, and we value participation from those who bring different perspectives to our community. We actively encourage participation from those who come from historically underrepresented groups in software development and STEM.\n\n\n\nBe friendly and patient: Remember that you might not be communicating in someone else’s primary spoken or programming language, and that we don’t all have the same background knowledge and experience.\nBe helpful: Offer constructive feedback on others’ ideas and code.\nBe collaborative: Collaboration improves the efficiency and quality of our work. We aim to understand the needs of other users, and to work transparently with others to coordinate our efforts.\nBe considerate and respectful: There is no excuse for disrespectful behavior. When making comments in public spaces, we should consider the potential impact on the whole community. For instance, jokes/teasing may seem harmless among users who are familiar with one another, but this can be intimidating and off-putting for other community members.\nAsk for help and seek feedback: We’re all learning, and no one is expected to be perfect. Asking questions and seeking feedback early on can help us work efficiently and avoid bigger problems in the future.\nTake responsibility for your actions: We all make mistakes; when we do, we take responsibility for them. If someone has been harmed or offended, we listen carefully and respectfully, and work to improve our behavior.\nValue decisiveness, clarity, and consensus: We hope that jsPsych contributors can resolve disagreements constructively, using clear rationale for their perspective, and with humility. When they cannot, the project leader can offer clarity and direction.\n\n\n\n\nThe community’s core principles apply to all private and public spaces where the jsPsych project is officially managed, promoted, or discussed. Examples include all of the jsPsych-related GitHub repos (issues, commit messages, pull requests, etc.), the jsPsych Google Group, and anyone acting as a representative of the project, whether in person or online, including via social media and email. We expect this Code of Conduct to be honoured by everyone in the jsPsych community."
  },
  {
    "objectID": "jspsych/code-of-conduct.html#leadership",
    "href": "jspsych/code-of-conduct.html#leadership",
    "title": "jsPsych Code of Conduct",
    "section": "",
    "text": "We encourage all participants in the jsPsych community to feel empowered to lead, to take action, and to experiment when they feel that innovation could improve the project. There is no need to wait for delegation or permission - leadership can be exercised by anyone simply by taking action.\nAlthough we strive to make jsPsych as open and collaborative as possible, it’s important to have a project leader in order to ensure that jsPsych remains (1) as accessible as possible for all users, (2) stable and reliable, (3) easy to maintain, (4) internally consistent, etc.\nSuch leadership occasionally requires decisions that are in the best interest of the project but may not be understood by or beneficial to everyone. These decisions are important because they keep the project on track, and enable it to move forward faster than if we required complete consensus. Any major decisions taken by the project leader should be mindful of the challenges they may present for others. We expect the project leader to communicate major decisions/changes early on, and to provide their reasoning.\n\n\nWe invite anyone to contribute to any aspect of the project. You can find more information about contributing here: https://www.jspsych.org/latest/developers/contributing/\nBecause changes to the jsPsych library can have a huge potential for impact on all users, any proposed changes to the library have to be considered very carefully. For this reason, it is always a good idea to check with the project leader about whether your proposed contribution would likely be merged into the jsPsych library. Keep in mind that, even if your work isn’t suitable for merging into the jsPsych library, you can still use it in your own projects and share it publicly with others, e.g. via a separate GitHub repository.\nContributors are acknowledged in the contributors.md file in the jsPsych GitHub repo, and in the notes for each release. If you feel that your contribution has been overlooked, please contact Josh de Leeuw (josh.deleeuw@gmail.com). We also encourage opening a pull request to modify the contributors.md file.\n\n\n\nWe expect jsPsych users and developers to be aware when they are conflicted due to employment or other projects they are involved in, and to abstain or delegate decisions that may be seen to be self-interested. We expect that everyone who participates in the jsPsych community does so with the goal of improving the library for all of its users."
  },
  {
    "objectID": "jspsych/code-of-conduct.html#reporting-code-of-conduct-issues",
    "href": "jspsych/code-of-conduct.html#reporting-code-of-conduct-issues",
    "title": "jsPsych Code of Conduct",
    "section": "",
    "text": "In the event that a Code of Conduct issue cannot be resolved among users, please feel free to report your concerns to Josh de Leeuw (josh.deleeuw@gmail.com). If Josh de Leeuw is involved in the issue, you can report it to Becky Gilbert instead (beckyannegilbert@gmail.com). In your report, please include as much information as possible about what occurred, including any relevant links/attachments.\nAll reports will be reviewed by a multi-person team and will result in a response that is deemed necessary and appropriate to the circumstances. Where additional perspectives are needed, the team may seek insight from others with relevant expertise or experience. The confidentiality of the person reporting the incident will be kept at all times. Involved parties are never part of the review team. Anyone asked to stop unacceptable behavior is expected to comply immediately. If an individual engages in unacceptable behavior, the review team may take any action they deem appropriate, including a permanent ban from the community.\nThis code is not exhaustive or complete. It serves to capture our common understanding of a welcoming, productive, and collaborative environment. We expect the code to be followed in spirit as much as in the letter."
  },
  {
    "objectID": "jspsych/code-of-conduct.html#references",
    "href": "jspsych/code-of-conduct.html#references",
    "title": "jsPsych Code of Conduct",
    "section": "",
    "text": "This document was based on, or inspired by, the following Codes of Conduct: * Ubuntu https://ubuntu.com/community/code-of-conduct * Microsoft https://opensource.microsoft.com/codeofconduct/ * Contributor Covenant https://www.contributor-covenant.org/version/1/4/code-of-conduct"
  },
  {
    "objectID": "jspsych/VERSION.html",
    "href": "jspsych/VERSION.html",
    "title": "Blog",
    "section": "",
    "text": "Included in this release:\n\n\n\n\n\n\n\n\nPackage\nVersion\nDocumentation\n\n\n\n\njspsych\n7.3.3\nhttps://www.jspsych.org/7.3/\n\n\nextension-mouse-tracking\n1.0.2\nhttps://www.jspsych.org/7.3/extensions/mouse-tracking\n\n\nextension-record-video\n1.0.1\nhttps://www.jspsych.org/7.3/extensions/record-video\n\n\nextension-webgazer\n1.0.2\nhttps://www.jspsych.org/7.3/extensions/webgazer\n\n\nplugin-animation\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/animation\n\n\nplugin-audio-button-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/audio-button-response\n\n\nplugin-audio-keyboard-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/audio-keyboard-response\n\n\nplugin-audio-slider-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/audio-slider-response\n\n\nplugin-browser-check\n1.0.2\nhttps://www.jspsych.org/7.3/plugins/browser-check\n\n\nplugin-call-function\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/call-function\n\n\nplugin-canvas-button-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/canvas-button-response\n\n\nplugin-canvas-keyboard-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/canvas-keyboard-response\n\n\nplugin-canvas-slider-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/canvas-slider-response\n\n\nplugin-categorize-animation\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/categorize-animation\n\n\nplugin-categorize-html\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/categorize-html\n\n\nplugin-categorize-image\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/categorize-image\n\n\nplugin-cloze\n1.2.0\nhttps://www.jspsych.org/7.3/plugins/cloze\n\n\nplugin-external-html\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/external-html\n\n\nplugin-free-sort\n1.0.2\nhttps://www.jspsych.org/7.3/plugins/free-sort\n\n\nplugin-fullscreen\n1.2.0\nhttps://www.jspsych.org/7.3/plugins/fullscreen\n\n\nplugin-html-audio-response\n1.0.2\nhttps://www.jspsych.org/7.3/plugins/html-audio-response\n\n\nplugin-html-button-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/html-button-response\n\n\nplugin-html-keyboard-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/html-keyboard-response\n\n\nplugin-html-slider-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/html-slider-response\n\n\nplugin-html-video-response\n1.0.1\nhttps://www.jspsych.org/7.3/plugins/html-video-response\n\n\nplugin-iat-html\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/iat-html\n\n\nplugin-iat-image\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/iat-image\n\n\nplugin-image-button-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/image-button-response\n\n\nplugin-image-keyboard-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/image-keyboard-response\n\n\nplugin-image-slider-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/image-slider-response\n\n\nplugin-initialize-camera\n1.0.1\nhttps://www.jspsych.org/7.3/plugins/initialize-camera\n\n\nplugin-initialize-microphone\n1.0.2\nhttps://www.jspsych.org/7.3/plugins/initialize-microphone\n\n\nplugin-instructions\n1.1.3\nhttps://www.jspsych.org/7.3/plugins/instructions\n\n\nplugin-maxdiff\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/maxdiff\n\n\nplugin-mirror-camera\n1.0.1\nhttps://www.jspsych.org/7.3/plugins/mirror-camera\n\n\nplugin-preload\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/preload\n\n\nplugin-reconstruction\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/reconstruction\n\n\nplugin-resize\n1.0.2\nhttps://www.jspsych.org/7.3/plugins/resize\n\n\nplugin-same-different-html\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/same-different-html\n\n\nplugin-same-different-image\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/same-different-image\n\n\nplugin-serial-reaction-time-mouse\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/serial-reaction-time-mouse\n\n\nplugin-serial-reaction-time\n1.1.3\nhttps://www.jspsych.org/7.3/plugins/serial-reaction-time\n\n\nplugin-sketchpad\n1.0.3\nhttps://www.jspsych.org/7.3/plugins/sketchpad\n\n\nplugin-survey-html-form\n1.0.2\nhttps://www.jspsych.org/7.3/plugins/survey-html-form\n\n\nplugin-survey-likert\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/survey-likert\n\n\nplugin-survey-multi-choice\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/survey-multi-choice\n\n\nplugin-survey-multi-select\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/survey-multi-select\n\n\nplugin-survey-text\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/survey-text\n\n\nplugin-survey\n0.2.1\nhttps://www.jspsych.org/7.3/plugins/survey\n\n\nplugin-video-button-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/video-button-response\n\n\nplugin-video-keyboard-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/video-keyboard-response\n\n\nplugin-video-slider-response\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/video-slider-response\n\n\nplugin-virtual-chinrest\n2.0.2\nhttps://www.jspsych.org/7.3/plugins/virtual-chinrest\n\n\nplugin-visual-search-circle\n1.1.2\nhttps://www.jspsych.org/7.3/plugins/visual-search-circle\n\n\nplugin-webgazer-calibrate\n1.0.2\nhttps://www.jspsych.org/7.3/plugins/webgazer-calibrate\n\n\nplugin-webgazer-init-camera\n1.0.2\nhttps://www.jspsych.org/7.3/plugins/webgazer-init-camera\n\n\nplugin-webgazer-validate\n1.0.2\nhttps://www.jspsych.org/7.3/plugins/webgazer-validate"
  },
  {
    "objectID": "jspsych/examples/README-template.html",
    "href": "jspsych/examples/README-template.html",
    "title": "Blog",
    "section": "",
    "text": "jspsych logo\njsPsych is a JavaScript framework for creating behavioral experiments that run in a web browser."
  },
  {
    "objectID": "jspsych/examples/README-template.html#plugin-description",
    "href": "jspsych/examples/README-template.html#plugin-description",
    "title": "Blog",
    "section": "Plugin Description",
    "text": "Plugin Description\nThe XXXXXX plugin does XXXXXX."
  },
  {
    "objectID": "jspsych/examples/README-template.html#examples",
    "href": "jspsych/examples/README-template.html#examples",
    "title": "Blog",
    "section": "Examples",
    "text": "Examples\nSeveral example experiments and plugin demonstrations are available in the /examples folder. After you’ve downloaded the latest release, double-click on an example HTML file to run it in your web browser, and open it with a programming-friendly text editor to see how it works."
  },
  {
    "objectID": "jspsych/examples/README-template.html#documentation",
    "href": "jspsych/examples/README-template.html#documentation",
    "title": "Blog",
    "section": "Documentation",
    "text": "Documentation\nDocumentation for this plugin is available here."
  },
  {
    "objectID": "jspsych/examples/README-template.html#getting-help",
    "href": "jspsych/examples/README-template.html#getting-help",
    "title": "Blog",
    "section": "Getting help",
    "text": "Getting help\nFor questions about using the library, please use the GitHub discussions forum. You can also browse through the history of Q&A on the forum to find related questions."
  },
  {
    "objectID": "jspsych/examples/README-template.html#contributing",
    "href": "jspsych/examples/README-template.html#contributing",
    "title": "Blog",
    "section": "Contributing",
    "text": "Contributing\nWe :heart: contributions! See the contributing to jsPsych documentation page for more information about how you can help."
  },
  {
    "objectID": "jspsych/examples/README-template.html#citation",
    "href": "jspsych/examples/README-template.html#citation",
    "title": "Blog",
    "section": "Citation",
    "text": "Citation\nIf you use this library in academic work, the preferred citation is:\nde Leeuw, J.R., Gilbert, R.A., & Luchterhandt, B. (2023). jsPsych: Enabling an open-source collaborative ecosystem of behavioral experiments. Journal of Open Source Software, 8(85), 5351, https://joss.theoj.org/papers/10.21105/joss.05351.\nThis paper is an updated description of jsPsych and includes all current core team members. It replaces the earlier paper that described jsPsych:\nde Leeuw, J.R. (2015). jsPsych: A JavaScript library for creating behavioral experiments in a Web browser. Behavior Research Methods, 47(1), 1-12. doi:10.3758/s13428-014-0458-y\nCitations help us demonstrate that this library is used and valued, which allows us to continue working on it."
  },
  {
    "objectID": "jspsych/examples/README-template.html#contributors",
    "href": "jspsych/examples/README-template.html#contributors",
    "title": "Blog",
    "section": "Contributors",
    "text": "Contributors\njsPsych is open source project with numerous contributors. The project is currently managed by the core team of Josh de Leeuw (@jodeleeuw), Becky Gilbert (@becky-gilbert), and Björn Luchterhandt (@bjoluc).\njsPsych was created by Josh de Leeuw.\nWe’re also grateful for the generous support from a Mozilla Open Source Support award, which funded development of the library from 2020-2021."
  },
  {
    "objectID": "posts/stroop/index.html",
    "href": "posts/stroop/index.html",
    "title": "Stroop",
    "section": "",
    "text": "Here I will be creating two stroop experiments. One is standard and the other manipulates the brightness of the colors of the words. In this manipulated experiment, the words have an opacity value of 0.1. The link below contains both experiments which will be given to subjects in a random order.\nHTML Page"
  },
  {
    "objectID": "posts/html-javascript/index.html",
    "href": "posts/html-javascript/index.html",
    "title": "HTML/JavaScript",
    "section": "",
    "text": "Here I will be creating a simple interactive html page using html and javascript.\nHTML Page\nCSS"
  },
  {
    "objectID": "posts/reaction-time/index.html",
    "href": "posts/reaction-time/index.html",
    "title": "Reaction Time",
    "section": "",
    "text": "Here I will be creating reaction time expirements based on Donders model.\nReaction Time Test\nLoad Images"
  },
  {
    "objectID": "posts/new-post/index.html",
    "href": "posts/new-post/index.html",
    "title": "New Post",
    "section": "",
    "text": "Hello! This is my first post"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Picture Memorability\n\n\n\n\n\n\n\npicture memory\n\n\nliterature\n\n\ncomputer vision\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2023\n\n\nCobe Liu\n\n\n\n\n\n\n  \n\n\n\n\nImage Orientation\n\n\n\n\n\n\n\norientation\n\n\npicture memory\n\n\nliterature\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nCobe Liu\n\n\n\n\n\n\n  \n\n\n\n\nPicture Memory Review\n\n\n\n\n\n\n\nmemory\n\n\nresearch\n\n\nacademic literature\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nCobe Liu\n\n\n\n\n\n\n  \n\n\n\n\nPicture Memory\n\n\n\n\n\n\n\ncode\n\n\nwebsite\n\n\nmemory\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\nCobe Liu\n\n\n\n\n\n\n  \n\n\n\n\nData Analysis\n\n\n\n\n\n\n\ncode\n\n\nwebsite\n\n\nstroop\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nCobe Liu\n\n\n\n\n\n\n  \n\n\n\n\nStroop\n\n\n\n\n\n\n\ncode\n\n\nwebsite\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nCobe Liu\n\n\n\n\n\n\n  \n\n\n\n\nReaction Time\n\n\n\n\n\n\n\ncode\n\n\nwebsite\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\nCobe Liu\n\n\n\n\n\n\n  \n\n\n\n\nHTML/JavaScript\n\n\n\n\n\n\n\ncode\n\n\nwebsite\n\n\npsychology\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nCobe Liu\n\n\n\n\n\n\n  \n\n\n\n\nNew Post\n\n\n\n\n\n\n\nthoughts\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nCobe Liu\n\n\n\n\n\n\nNo matching items"
  }
]